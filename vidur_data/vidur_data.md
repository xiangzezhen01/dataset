## **1、Structure of Profiling data**  
The profiling data is stored in the data/profiling directory. The profiling data is stored in CSV format. The profiling data is stored in the following format:
### directory
```yaml
data/
  ├── compute/          # Compute operator data (model- and hardware-dependent)
  │   └── {device}/    # Hardware type (e.g., a100/h100)
  │       └── {model}/  # Hugging Face model ID (e.g., meta-llama/Llama-2-70b-hf)
  │           ├── mlp.csv       # MLP layer runtime data
  │           └── attention.csv # Attention layer runtime data
  └── network/          # Communication operator data (hardware topology-dependent)
      └── {network_device}/ # Hardware topology (e.g., a100_pair_nvlink/h100_dgx)
          ├── allreduce.csv   # All-reduce communication latency
          └── send_recv.csv   # Send-receive (pipeline parallelism) latency
```
- **Compute Data**：Performance data for MLP and attention operators of different models on the same hardware (e.g., A100) are stored independently.  
- **Communication Data**：Data for the same hardware topology (e.g., 4x A100 with pairwise NVLink) can be reused across all models.
### information

| **name**       | **Range**                                                                                                                  |
|----------------|----------------------------------------------------------------------------------------------------------------------------|
| device         | a40, a100, h100                                                                                                            |
| LLM            | Meta-Llama-3-70B, Meta-Llama-3-8B, CodeLlama-34b-Instruct-hf, InternLM-20b, Llama-2-7b-hf, Llama-2-70b-hf, phi-2, Qwen-72B |
| network_device | a40_pairwise_nvlink, a100_dgx, a100_pairwise_nvlink, h100_dgx, h100_pairwise_nvlink                                        |

| **file**       | **data_count per file** | **n_files** | **n_config** | **n_perf** |
|----------------|-------------------------|-------------|--------------|------------|
| attention.csv  | 58633                   | 20          | 11           | 5          |
| mlp.csv        | 1045                    | 20          | 8            | 10         |
| all_reduce.csv | (1989,6959)             | 5           | 6            | 1          |
| send_recv.csv  | 1989                    | 5           | 6            | 1          |


## **2、configurations and performance**  
### Attention layer runtime data in `compute`
Here is the analysis of each column in the `attention.csv` file, categorized into **Performance Metrics** and **Configuration Parameters** based on the paper's context:

#### **Table 1: Performance Metrics (Time Statistics)**  
| **Column Name**                                                                 | **Data Type** | **Description**                                                                 |
|-------------------------------------------------------------------------------|---------------|---------------------------------------------------------------------------------|
| time_stats.attn_input_reshape.min    | Time (s)      | Minimum time for attention input reshaping operation.                           |
| time_stats.attn_input_reshape.max    | Time (s)      | Maximum time for attention input reshaping operation.                           |
| time_stats.attn_input_reshape.mean   | Time (s)      | Average time for attention input reshaping operation.                           |
| time_stats.attn_input_reshape.median | Time (s)      | Median time for attention input reshaping operation.                           |
| time_stats.attn_input_reshape.std    | Time (s)      | Standard deviation of attention input reshaping time.                           |
| time_stats.attn_decode.min           | Time (s)      | Minimum time for decode-phase attention operation.                             |
| time_stats.attn_decode.max           | Time (s)      | Maximum time for decode-phase attention operation.                             |
| time_stats.attn_decode.mean          | Time (s)      | Average time for decode-phase attention operation.                             |
| time_stats.attn_decode.median        | Time (s)      | Median time for decode-phase attention operation.                             |
| time_stats.attn_decode.std           | Time (s)      | Standard deviation of decode-phase attention time.                             |
| time_stats.attn_output_reshape.min   | Time (s)      | Minimum time for attention output reshaping operation.                          |
| time_stats.attn_output_reshape.max   | Time (s)      | Maximum time for attention output reshaping operation.                          |
| time_stats.attn_output_reshape.mean  | Time (s)      | Average time for attention output reshaping operation.                          |
| time_stats.attn_output_reshape.median| Time (s)      | Median time for attention output reshaping operation.                          |
| time_stats.attn_output_reshape.std   | Time (s)      | Standard deviation of attention output reshaping time.                          |
| time_stats.attn_prefill.min          | Time (s)      | Minimum time for prefill-phase attention operation (0 in decode-phase data).    |
| time_stats.attn_prefill.max          | Time (s)      | Maximum time for prefill-phase attention operation (0 in decode-phase data).    |
| time_stats.attn_prefill.mean         | Time (s)      | Average time for prefill-phase attention operation (0 in decode-phase data).   |
| time_stats.attn_prefill.median       | Time (s)      | Median time for prefill-phase attention operation (0 in decode-phase data).   |
| time_stats.attn_prefill.std          | Time (s)      | Standard deviation of prefill-phase attention time (0 in decode-phase data).  |


#### **Table 2: Configuration Parameters**  
| **Column Name**                                                                 | **Data Type** | **Description**                                                                 |
|-------------------------------------------------------------------------------|---------------|---------------------------------------------------------------------------------|
| n_embd                                                                          | Integer       | Model embedding dimension (hidden layer size, e.g., 8192 for CodeLlama-34B).   |
| n_q_head                                                                       | Integer       | Number of query attention heads (e.g., 64 in Grouped Query Attention).         |
| n_kv_head                                                                      | Integer       | Number of key/value attention heads (e.g., 8 in Grouped Query Attention).      |
| block_size                                                                      | Integer       | Tokens processed per batch or sequence length (e.g., 16).                       |
| num_tensor_parallel_workers                                                   | Integer       | Number of GPUs for tensor parallelism (1 = no TP, single-GPU).                 |
| max_model_len                                                                   | Integer       | Maximum sequence length supported (e.g., 4096 for long-context models).       |
| batch_size                                                                      | Integer       | Number of requests per batch (tested values: 1, 2, 3).                        |
| prefill_chunk_size                                                             | Integer       | Chunk size for prefill phase (0 = no chunking in decode phase).                |
| kv_cache_size                                                                   | Integer       | Size of KV-Cache (historical tokens in decode phase, e.g., 32).              |
| is_prefill                                                                      | Boolean       | `True` for prefill phase, `False` for decode phase (current data: `False`).   |
| attention_backend                                                               | Enum          | Attention implementation (e.g., `FLASH_ATTENTION` for efficient memory access).|



### MLP layer runtime data
Here is the analysis of each column in the `mlp.csv` file, categorized into **Performance Metrics** and **Configuration Parameters** based on the paper's context:


#### **Table 1: Performance Metrics (Time Statistics for MLP and Related Layers)**  
| **Column Name**                                                                 | **Data Type** | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | **Relevance to the Paper**                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|-------------------------------------------------------------------------------|---------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| time_stats.emb.min/max/mean/median/std                                        | Time (s)      | Timing statistics for the embedding layer (converts tokens to embeddings).<br>- Example values (~0.36-0.4s) reflect fixed latency for token embedding, which is a lightweight operation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | The paper mentions embedding layers as part of the transformer architecture (§3.3), with fixed compute cost per token.                                                                                                                                                                                                                                                                                                                                                                                                                          |
| time_stats.input_layernorm.min/max/mean/median/std                            | Time (s)      | Timing for input layer normalization (applied before attention/MLP).<br>- Low variance (std ~0.003s) indicates stable performance, consistent with simple point-wise operations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Layer normalization is a token-level operator (§4.1), the paper states their runtime depends only on total token count.                                                                                                                                                                                                                                                                                                                                                                                                                       |
| time_stats.attn_pre_proj.min/max/mean/median/std                              | Time (s)      | Timing for attention pre-projection (linear layer to split hidden states into Query/Key/Value).<br>- Higher latency (~5.8s) reflects matrix multiplication complexity (proportional to `n_embd * n_head`).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Pre-projection is a key part of attention setup (§3.5), classified as a token-level operator in the paper.                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| time_stats.attn_rope.min/max/mean/median/std                                   | Time (s)      | Timing for RoPE (Rotary Position Embedding) application.<br>- Minimal variance (std ~0.001s) for this lightweight positional encoding operation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | RoPE is a sequence-level operation but with low compute cost, as noted in transformer architecture discussions (§3.3).                                                                                                                                                                                                                                                                                                                                                                                                                           |
| time_stats.attn_post_proj.min/max/mean/median/std                             | Time (s)      | Timing for attention post-projection (linear layer to merge multi-head outputs).<br>- Latency ~4.6s matches pre-projection complexity but with slightly lower variance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Post-projection is another token-level linear operation (§3.5), critical for combining attention outputs.                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| time_stats.post_attention_layernorm.min/max/mean/median/std                   | Time (s)      | Timing for post-attention layer normalization.<br>- Stable performance (std ~0.002s) similar to input layernorm.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Layer normalization after attention ensures numerical stability, modeled as a simple operator in the paper (§4.1).                                                                                                                                                                                                                                                                                                                                                                                                                            |
| time_stats.mlp_up_proj.min/max/mean/median/std                                 | Time (s)      | Timing for MLP up-projection (expands hidden dim from `n_embd` to `n_expanded_embd`).<br>- Highest latency (~26-35s) due to large matrix multiplication (8192 → 22016 dims).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | MLP up-projection is the most compute-intensive part of the transformer block (§3.3), aligning with the paper's focus on optimizing compute-bound operations.                                                                                                                                                                                                                                                                                                                                                                                         |
| time_stats.mlp_act.min/max/mean/median/std                                    | Time (s)      | Timing for MLP activation function (e.g., SwiGLU).<br>- Low latency (~0.98s) for non-linear activation, a lightweight point-wise operation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Activation functions are token-level operators (§4.1), their runtime depends only on token count.                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| time_stats.mlp_down_proj.min/max/mean/median/std                               | Time (s)      | Timing for MLP down-projection (reduces dim from `n_expanded_embd` to `n_embd`).<br>- Latency ~12.6s, half of up-projection time (22016 → 8192 dims).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Down-projection is a symmetric operation to up-projection, critical for MLP efficiency (§3.3).                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| time_stats.add.min/max/mean/median/std                                        | Time (s)      | Timing for residual connection addition (adds input to attention/MLP output).<br>- Negligible latency (~0.35s), a simple element-wise operation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Residual connections are fundamental to transformer architecture (§3.3), with minimal computational cost as noted in the paper.                                                                                                                                                                                                                                                                                                                                                                                                                  |


#### **Table 2: Configuration Parameters**  
| **Column Name**                                                                 | **Data Type** | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | **Relevance to the Paper**                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|-------------------------------------------------------------------------------|---------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| n_head                                                                        | Integer       | Number of attention heads (query heads, e.g., 64 in CodeLlama-34B).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Matches the `n_q_head` parameter in attention profiling, critical for parallelism and memory usage (§4.3).                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| n_kv_head                                                                     | Integer       | Number of key/value heads (e.g., 8 in GQA, fewer than query heads to reduce KV-Cache).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Aligns with the paper's discussion of Grouped Query Attention (GQA) to optimize memory (§7.3).                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| n_embd                                                                        | Integer       | Model hidden dimension (e.g., 8192), defining the size of each transformer layer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | A core model configuration parameter (§"How to add a new model"), directly impacting compute complexity.                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| n_expanded_embd                                                               | Integer       | Expanded dimension in MLP (e.g., 22016 = 8192 × 2.7), typical for SwiGLU activation in CodeLlama.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | MLP expansion ratio is a model-specific parameter (§3.3), affecting up-projection/down-projection compute cost.                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| vocab_size                                                                    | Integer       | Vocabulary size (e.g., 32768), determining embedding table size.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Influences embedding layer memory usage, though not a primary focus in the paper's performance modeling.                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| use_gated_mlp                                                                 | Boolean       | Whether to use gated MLP (e.g., SwiGLU activation, `True` in CodeLlama).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Gated MLPs are more efficient than standard MLPs (§3.3), the paper may compare gated/non-gated configurations.                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| num_tokens                                                                    | Integer       | Number of tokens processed per batch (e.g., 4096), reflecting workload sequence length.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Directly impacts batch-level compute time, as seen in the paper's workload analysis (§5.2).                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| num_tensor_parallel_workers                                                   | Integer       | Number of GPUs for tensor parallelism (1 = single-GPU, no TP).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | TP parallelism splits layers across GPUs (§4.6), the paper evaluates TP effects on MLP compute (§7.2).                                                                                                                                                                                                                                                                                                                                                                                                                                            |

### All-reduce communication latency data in `network`
Here is the analysis of each column in the `allreduce.csv` file, categorized into **Performance Metrics** and **Configuration Parameters** based on the paper's context:

#### **Table 1: Performance Metrics (All-Reduce Communication Latency)**  
| **Column Name**                                                                 | **Data Type** | **Description**                                                                 |
|-------------------------------------------------------------------------------|---------------|---------------------------------------------------------------------------------|
| time_stats.all_reduce.min          | Time (s)      | Minimum latency for the all_reduce operation.                                   |
| time_stats.all_reduce.max          | Time (s)      | Maximum latency for the all_reduce operation.                                   |
| time_stats.all_reduce.mean         | Time (s)      | Average latency for the all_reduce operation.                                  |
| time_stats.all_reduce.median       | Time (s)      | Median latency for the all_reduce operation.                                    |
| time_stats.all_reduce.std          | Time (s)      | Standard deviation of all_reduce latency (measures consistency).                |


#### **Table 2: Configuration Parameters (All-Reduce Setup)**  
| **Column Name**                                                                 | **Data Type** | **Description**                                                                 |
|-------------------------------------------------------------------------------|---------------|---------------------------------------------------------------------------------|
| rank                                                                          | Integer       | Rank of the current GPU in the network (0-based index in multi-GPU setup).       |
| num_workers                                                                   | Integer       | Total number of GPUs participating in the all_reduce operation (e.g., TP degree).|
| size                                                                          | Integer       | Data size (in bytes) being reduced across GPUs.                                 |
| collective                                                                    | String        | Type of collective operation (`all_reduce` for tensor parallelism).             |
| devices_per_node                                                              | Integer       | Number of GPUs per node (e.g., `1` node with 8 GPUs in `a100_dgx`).             |
| max_devices_per_node                                                          | Integer       | Maximum GPUs per node (e.g., `8` for a DGX node with full NVLink connectivity). |



### Send-receive (pipeline parallelism) latency data in `network`
Here is the analysis of each column in the `send_recv.csv` file, categorized into **Performance Metrics** and **Configuration Parameters** based on the paper's context:


#### **Table 1: Performance Metrics (Network Communication)**  
| **Column Name**                                                                 | **Data Type** | **Description**                                                                 |
|-------------------------------------------------------------------------------|---------------|---------------------------------------------------------------------------------|
| time_stats.send_recv.min           | Time (s)      | Minimum latency for send_recv operation.                                        |
| time_stats.send_recv.max           | Time (s)      | Maximum latency for send_recv operation.                                        |
| time_stats.send_recv.mean          | Time (s)      | Average latency for send_recv operation.                                       |
| time_stats.send_recv.median        | Time (s)      | Median latency for send_recv operation.                                         |
| time_stats.send_recv.std           | Time (s)      | Standard deviation of send_recv latency.                                        |


#### **Table 2: Configuration Parameters (Network Setup)**  
| **Column Name**                                                                 | **Data Type** | **Description**                                                                 |
|-------------------------------------------------------------------------------|---------------|---------------------------------------------------------------------------------|
| rank                                                                          | Integer       | GPU rank in the network topology (0-based index).                                |
| num_workers                                                                   | Integer       | Total number of GPUs involved in the operation (e.g., pipeline stages).         |
| size                                                                          | Integer       | Data transfer size (bytes) between GPUs.                                        |
| collective                                                                    | String        | Type of communication operation (`send_recv` for pipeline parallelism).         |
| devices_per_node                                                              | Integer       | Number of GPUs per node in the cluster.                                         |
| max_devices_per_node                                                          | Integer       | Maximum GPUs supported per node (e.g., 8 for a DGX node).                        |







